#!/usr/bin/env python3
"""
Error Context v1.0 ‚Äî –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—à–∏–±–æ–∫ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–ú–æ–¥—É–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –æ—à–∏–±–æ–∫:
- –°–æ–∑–¥–∞–Ω–∏–µ TXT —Ñ–∞–π–ª–æ–≤ –∏–∑ DOCX –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–≤
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π
- –°–æ–∑–¥–∞–Ω–∏–µ alignment.json —Å —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
- –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ —Å –ø–æ–∑–∏—Ü–∏—è–º–∏ –≤–æ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python error_context.py prepare 01       # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è –≥–ª–∞–≤—ã 1
    python error_context.py prepare --all    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –≤—Å–µ –≥–ª–∞–≤—ã
    python error_context.py analyze 01       # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –≥–ª–∞–≤—ã 1

v1.0 (2026-01-31): –ù–∞—á–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è
    - –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è DOCX ‚Üí TXT
    - –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ ErrorContext dataclass
"""

VERSION = '1.0.0'
VERSION_DATE = '2026-01-31'

import argparse
import json
import os
import re
import uuid
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple


# =============================================================================
# –ü–£–¢–ò –ö –§–ê–ô–õ–ê–ú
# =============================================================================

BASE_DIR = Path(__file__).parent.parent
ORIGINALS_DIR = BASE_DIR / '–û—Ä–∏–≥–∏–Ω–∞–ª' / '–ì–ª–∞–≤—ã'
RESULTS_DIR = BASE_DIR / '–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏'
TRANSCRIPTIONS_DIR = BASE_DIR / '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏'


# =============================================================================
# –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø DOCX ‚Üí TXT
# =============================================================================

def docx_to_txt(docx_path: Path) -> str:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç DOCX –≤ plain text.

    Args:
        docx_path: –ø—É—Ç—å –∫ DOCX —Ñ–∞–π–ª—É

    Returns:
        –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    try:
        from docx import Document
    except ImportError:
        raise ImportError("–¢—Ä–µ–±—É–µ—Ç—Å—è python-docx: pip install python-docx")

    doc = Document(str(docx_path))
    paragraphs = []

    for para in doc.paragraphs:
        text = para.text.strip()
        if text:
            paragraphs.append(text)

    return '\n\n'.join(paragraphs)


def normalize_text_for_comparison(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:
    - –ü—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    - –ó–∞–º–µ–Ω—è–µ—Ç —ë ‚Üí –µ
    - –£–±–∏—Ä–∞–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é (–∫—Ä–æ–º–µ –¥–µ—Ñ–∏—Å–æ–≤ –≤ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö)
    - –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–±–µ–ª—ã

    Args:
        text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    # –ù–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    text = text.lower()

    # —ë ‚Üí –µ
    text = text.replace('—ë', '–µ')

    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –æ—Å—Ç–∞–≤–ª—è—è –¥–µ—Ñ–∏—Å—ã –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤
    # –°–Ω–∞—á–∞–ª–∞ –∑–∞—â–∏—â–∞–µ–º –¥–µ—Ñ–∏—Å—ã –º–µ–∂–¥—É –±—É–∫–≤–∞–º–∏
    text = re.sub(r'(\w)-(\w)', r'\1HYPHEN\2', text)

    # –£–±–∏—Ä–∞–µ–º –≤—Å—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    text = re.sub(r'[^\w\s]', ' ', text)

    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ñ–∏—Å—ã
    text = text.replace('HYPHEN', '-')

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()

    return text


def text_to_words(text: str) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞.

    Args:
        text: —Ç–µ–∫—Å—Ç (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –∏–ª–∏ –Ω–µ—Ç)

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤
    """
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –µ—â—ë –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
    normalized = normalize_text_for_comparison(text)
    return normalized.split()


# =============================================================================
# –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•
# =============================================================================

@dataclass
class WordPosition:
    """–ü–æ–∑–∏—Ü–∏—è —Å–ª–æ–≤–∞ –≤ —Ñ–∞–π–ª–µ"""
    word_idx: int           # –ò–Ω–¥–µ–∫—Å —Å–ª–æ–≤–∞ –≤ –º–∞—Å—Å–∏–≤–µ —Å–ª–æ–≤
    char_start: int         # –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —Å–∏–º–≤–æ–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ
    char_end: int           # –ö–æ–Ω–µ—á–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —Å–∏–º–≤–æ–ª–∞
    word: str               # –°–∞–º–æ —Å–ª–æ–≤–æ
    word_normalized: str    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞


@dataclass
class AlignmentSegment:
    """–°–µ–≥–º–µ–Ω—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–µ–∂–¥—É –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π"""
    segment_id: int
    anchor_before: Optional[str]    # –Ø–∫–æ—Ä—å –¥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
    anchor_after: Optional[str]     # –Ø–∫–æ—Ä—å –ø–æ—Å–ª–µ —Å–µ–≥–º–µ–Ω—Ç–∞

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã (–∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏)
    time_start: float
    time_end: float

    # –ì—Ä–∞–Ω–∏—Ü—ã –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ (–∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤)
    original_start: int
    original_end: int

    # –ì—Ä–∞–Ω–∏—Ü—ã –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (–∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤)
    transcript_start: int
    transcript_end: int

    # ID –æ—à–∏–±–æ–∫ –≤ —ç—Ç–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ
    error_ids: List[str] = field(default_factory=list)


@dataclass
class ErrorLink:
    """–°–≤—è–∑—å –º–µ–∂–¥—É –æ—à–∏–±–∫–∞–º–∏ (–¥–ª—è merge/split –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤)"""
    link_id: str
    error_ids: List[str]            # ID —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
    link_type: str                  # merge_artifact, split_artifact
    pattern: str                    # "–Ω–∞+–≤—Å—Ç—Ä–µ—á—É=–Ω–∞–≤—Å—Ç—Ä–µ—á—É"
    original_parts: List[str]       # ["–Ω–∞", "–≤—Å—Ç—Ä–µ—á—É"]
    merged_form: Optional[str]      # "–Ω–∞–≤—Å—Ç—Ä–µ—á—É"
    confidence: float = 1.0


@dataclass
class ErrorContext:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""

    # –ò–î–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–Ø
    error_id: str                   # UUID –æ—à–∏–±–∫–∏
    chapter: int
    error_type: str                 # substitution, insertion, deletion

    # –°–õ–û–í–ê
    wrong: str                      # –ß—Ç–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª –Ø–Ω–¥–µ–∫—Å / —Å–∫–∞–∑–∞–ª —á—Ç–µ—Ü
    correct: str                    # –ß—Ç–æ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ

    # –í–†–ï–ú–ï–ù–ù–´–ï –û–ö–ù–ê
    time: float                     # –¢–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –æ—à–∏–±–∫–∏
    time_end: float                 # –ö–æ–Ω–µ—Ü —Å–ª–æ–≤–∞
    window_start: float             # –ù–∞—á–∞–ª–æ –æ–∫–Ω–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (—Å–µ–≥–º–µ–Ω—Ç)
    window_end: float               # –ö–æ–Ω–µ—Ü –æ–∫–Ω–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
    context_start: float            # –ù–∞—á–∞–ª–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (¬±N —Å–µ–∫)
    context_end: float              # –ö–æ–Ω–µ—Ü —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

    # –ü–û–ó–ò–¶–ò–ò –í –§–ê–ô–õ–ê–•
    pos_transcript: int             # –ò–Ω–¥–µ–∫—Å –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (–º–∞—Å—Å–∏–≤ —Å–ª–æ–≤)
    pos_transcript_char: int        # –ü–æ–∑–∏—Ü–∏—è —Å–∏–º–≤–æ–ª–∞ –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ TXT
    pos_normalized: int             # –ò–Ω–¥–µ–∫—Å –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º —Ñ–∞–π–ª–µ
    pos_original: int               # –ò–Ω–¥–µ–∫—Å –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ (–º–∞—Å—Å–∏–≤ —Å–ª–æ–≤)
    pos_original_char: int          # –ü–æ–∑–∏—Ü–∏—è —Å–∏–º–≤–æ–ª–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ TXT

    # –ö–û–ù–¢–ï–ö–°–¢–´ (¬±N —Å–ª–æ–≤)
    context_transcript: List[str]   # –°–ª–æ–≤–∞ –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    context_normalized: List[str]   # –°–ª–æ–≤–∞ –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ
    context_original: List[str]     # –°–ª–æ–≤–∞ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
    context_aligned: str            # –í–µ—Å—å —Å–µ–≥–º–µ–Ω—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è

    # –°–í–Ø–ó–ê–ù–ù–´–ï –û–®–ò–ë–ö–ò (–¥–ª—è —Å–ª–∏—è–Ω–∏—è/—Ä–∞–∑–±–∏–µ–Ω–∏—è)
    linked_error_ids: List[str] = field(default_factory=list)
    link_type: Optional[str] = None   # merge_artifact, split_artifact, None
    merged_form: Optional[str] = None # "–Ω–∞–≤—Å—Ç—Ä–µ—á—É" (–µ—Å–ª–∏ —ç—Ç–æ split)
    split_parts: List[str] = field(default_factory=list)  # ["–Ω–∞", "–≤—Å—Ç—Ä–µ—á—É"]

    # –ú–ï–¢–ê–î–ê–ù–ù–´–ï
    segment_id: int = -1            # ID —Å–µ–≥–º–µ–Ω—Ç–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
    is_boundary: bool = False       # –ù–∞ –≥—Ä–∞–Ω–∏—Ü–µ —Å–µ–≥–º–µ–Ω—Ç–∞?

    # –ú–û–†–§–û–õ–û–ì–ò–Ø –ò –°–ï–ú–ê–ù–¢–ò–ö–ê (–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–ª–µ–π)
    lemma_wrong: Optional[str] = None
    lemma_correct: Optional[str] = None
    pos_wrong: Optional[str] = None
    pos_correct: Optional[str] = None
    same_lemma: bool = False
    same_pos: bool = False
    semantic_similarity: float = 0.0
    phonetic_similarity: float = 0.0

    # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø
    is_filtered: bool = False
    filter_reason: Optional[str] = None
    is_golden: bool = False

    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è JSON"""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ErrorContext':
        """–°–æ–∑–¥–∞—ë—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        return cls(**data)


# =============================================================================
# –ü–û–î–ì–û–¢–û–í–ö–ê –§–ê–ô–õ–û–í
# =============================================================================

def get_chapter_paths(chapter: int) -> Dict[str, Path]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç–∏ –∫–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º –≥–ª–∞–≤—ã.

    Args:
        chapter: –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã (1-5)

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º
    """
    chapter_str = f"{chapter:02d}"
    results_chapter_dir = RESULTS_DIR / chapter_str

    # –ò—â–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª (—Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è)
    original_docx = None
    for pattern in [f'–ì–ª–∞–≤–∞ {chapter}.docx', f'–ì–ª–∞–≤–∞{chapter}.docx']:
        path = ORIGINALS_DIR / pattern
        if path.exists():
            original_docx = path
            break

    # –î–ª—è –≥–ª–∞–≤—ã 5 –º–æ–∂–µ—Ç –±—ã—Ç—å TXT
    original_txt_existing = ORIGINALS_DIR / f'–ì–ª–∞–≤–∞{chapter}.txt'

    return {
        'chapter': chapter,
        'chapter_str': chapter_str,

        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        'results_dir': results_chapter_dir,

        # –û—Ä–∏–≥–∏–Ω–∞–ª
        'original_docx': original_docx,
        'original_txt_existing': original_txt_existing if original_txt_existing.exists() else None,

        # –ù–æ–≤—ã–µ —Ñ–∞–π–ª—ã (–±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã)
        'original_txt': results_chapter_dir / f'{chapter_str}_original.txt',
        'original_normalized': results_chapter_dir / f'{chapter_str}_original_normalized.txt',
        'transcript_normalized': results_chapter_dir / f'{chapter_str}_transcript_normalized.txt',
        'alignment_json': results_chapter_dir / f'{chapter_str}_alignment.json',
        'context_json': results_chapter_dir / f'{chapter_str}_error_contexts.json',

        # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
        'compared_json': results_chapter_dir / f'{chapter_str}_compared.json',
        'filtered_json': results_chapter_dir / f'{chapter_str}_filtered.json',
        'transcript_json': results_chapter_dir / f'{chapter_str}_transcript.json',
    }


def prepare_chapter_files(chapter: int, force: bool = False) -> Dict[str, Any]:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –≥–ª–∞–≤—ã.

    –°–æ–∑–¥–∞—ë—Ç:
    - {chapter}_original.txt ‚Äî –æ—Ä–∏–≥–∏–Ω–∞–ª –∫–∞–∫ plain text
    - {chapter}_original_normalized.txt ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ—Ä–∏–≥–∏–Ω–∞–ª
    - {chapter}_transcript_normalized.txt ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è

    Args:
        chapter: –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã
        force: –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã

    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    """
    paths = get_chapter_paths(chapter)
    stats = {
        'chapter': chapter,
        'files_created': [],
        'files_skipped': [],
        'errors': [],
    }

    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ—Ç
    paths['results_dir'].mkdir(parents=True, exist_ok=True)

    # 1. –°–æ–∑–¥–∞—ë–º TXT –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
    original_text = None

    if paths['original_txt'].exists() and not force:
        stats['files_skipped'].append(str(paths['original_txt']))
        original_text = paths['original_txt'].read_text(encoding='utf-8')
    else:
        try:
            if paths['original_docx'] and paths['original_docx'].exists():
                original_text = docx_to_txt(paths['original_docx'])
                paths['original_txt'].write_text(original_text, encoding='utf-8')
                stats['files_created'].append(str(paths['original_txt']))
            elif paths['original_txt_existing']:
                # –ö–æ–ø–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π TXT
                original_text = paths['original_txt_existing'].read_text(encoding='utf-8')
                paths['original_txt'].write_text(original_text, encoding='utf-8')
                stats['files_created'].append(str(paths['original_txt']))
            else:
                stats['errors'].append(f"–ù–µ –Ω–∞–π–¥–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª –¥–ª—è –≥–ª–∞–≤—ã {chapter}")
        except Exception as e:
            stats['errors'].append(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ DOCX: {e}")

    # 2. –°–æ–∑–¥–∞—ë–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ—Ä–∏–≥–∏–Ω–∞–ª
    if original_text:
        if paths['original_normalized'].exists() and not force:
            stats['files_skipped'].append(str(paths['original_normalized']))
        else:
            normalized_original = normalize_text_for_comparison(original_text)
            paths['original_normalized'].write_text(normalized_original, encoding='utf-8')
            stats['files_created'].append(str(paths['original_normalized']))

    # 3. –°–æ–∑–¥–∞—ë–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏–∑ JSON
    if paths['transcript_json'].exists():
        if paths['transcript_normalized'].exists() and not force:
            stats['files_skipped'].append(str(paths['transcript_normalized']))
        else:
            try:
                transcript_text = extract_text_from_transcript(paths['transcript_json'])
                normalized_transcript = normalize_text_for_comparison(transcript_text)
                paths['transcript_normalized'].write_text(normalized_transcript, encoding='utf-8')
                stats['files_created'].append(str(paths['transcript_normalized']))
            except Exception as e:
                stats['errors'].append(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
    else:
        stats['errors'].append(f"–ù–µ –Ω–∞–π–¥–µ–Ω transcript.json –¥–ª—è –≥–ª–∞–≤—ã {chapter}")

    return stats


def extract_text_from_transcript(transcript_path: Path) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ JSON —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –Ø–Ω–¥–µ–∫—Å–∞.

    Args:
        transcript_path: –ø—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É

    Returns:
        –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    """
    with open(transcript_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    words = []

    # –§–æ—Ä–º–∞—Ç –Ø–Ω–¥–µ–∫—Å SpeechKit
    if 'chunks' in data:
        for chunk in data['chunks']:
            alternatives = chunk.get('alternatives', [])
            if alternatives:
                for word_data in alternatives[0].get('words', []):
                    words.append(word_data.get('word', ''))

    return ' '.join(words)


def extract_words_with_timing(transcript_path: Path) -> List[Dict[str, Any]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞ —Å —Ç–∞–π–º–∏–Ω–≥–∞–º–∏ –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.

    Args:
        transcript_path: –ø—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π {word, start_time, end_time, confidence}
    """
    with open(transcript_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    words = []

    if 'chunks' in data:
        for chunk in data['chunks']:
            alternatives = chunk.get('alternatives', [])
            if alternatives:
                for word_data in alternatives[0].get('words', []):
                    # –ü–∞—Ä—Å–∏–º –≤—Ä–µ–º—è (—Ñ–æ—Ä–º–∞—Ç "3.139s")
                    start_str = word_data.get('startTime', '0s')
                    end_str = word_data.get('endTime', '0s')

                    start_time = float(start_str.rstrip('s')) if start_str else 0.0
                    end_time = float(end_str.rstrip('s')) if end_str else 0.0

                    words.append({
                        'word': word_data.get('word', ''),
                        'start_time': start_time,
                        'end_time': end_time,
                        'confidence': word_data.get('confidence', 1.0),
                    })

    return words


# =============================================================================
# –ü–û–°–¢–†–û–ï–ù–ò–ï –ü–û–ó–ò–¶–ò–û–ù–ù–û–ì–û –ò–ù–î–ï–ö–°–ê
# =============================================================================

def build_word_positions(text: str) -> List[WordPosition]:
    """
    –°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å –ø–æ–∑–∏—Ü–∏–π –≤—Å–µ—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.

    Args:
        text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –°–ø–∏—Å–æ–∫ WordPosition –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
    """
    positions = []
    normalized_text = normalize_text_for_comparison(text)
    words = normalized_text.split()

    # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
    current_pos = 0
    for idx, word in enumerate(words):
        start = normalized_text.find(word, current_pos)
        if start == -1:
            start = current_pos
        end = start + len(word)

        positions.append(WordPosition(
            word_idx=idx,
            char_start=start,
            char_end=end,
            word=word,
            word_normalized=word.lower().replace('—ë', '–µ'),
        ))

        current_pos = end

    return positions


def find_word_position(
    word: str,
    positions: List[WordPosition],
    near_idx: int = -1,
    max_distance: int = 50
) -> Optional[int]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ —Å–ø–∏—Å–∫–µ –ø–æ–∑–∏—Ü–∏–π.

    Args:
        word: –∏—Å–∫–æ–º–æ–µ —Å–ª–æ–≤–æ
        positions: —Å–ø–∏—Å–æ–∫ WordPosition
        near_idx: –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è (–¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä—è–¥–æ–º)
        max_distance: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç near_idx

    Returns:
        –ò–Ω–¥–µ–∫—Å —Å–ª–æ–≤–∞ –∏–ª–∏ None
    """
    word_norm = word.lower().replace('—ë', '–µ')

    # –ï—Å–ª–∏ –µ—Å—Ç—å near_idx ‚Äî –∏—â–µ–º —Å–Ω–∞—á–∞–ª–∞ —Ä—è–¥–æ–º
    if near_idx >= 0:
        start = max(0, near_idx - max_distance)
        end = min(len(positions), near_idx + max_distance)

        for i in range(start, end):
            if positions[i].word_normalized == word_norm:
                return i

    # –ò–Ω–∞—á–µ ‚Äî –ø–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫
    for i, pos in enumerate(positions):
        if pos.word_normalized == word_norm:
            return i

    return None


# =============================================================================
# –£–¢–ò–õ–ò–¢–´
# =============================================================================

def generate_error_id() -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –æ—à–∏–±–∫–∏"""
    return str(uuid.uuid4())[:8]


def get_context_window(
    words: List[str],
    center_idx: int,
    window_size: int = 10
) -> List[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–∫–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ —Å–ª–æ–≤–∞.

    Args:
        words: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤
        center_idx: –∏–Ω–¥–µ–∫—Å —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        window_size: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    """
    start = max(0, center_idx - window_size)
    end = min(len(words), center_idx + window_size + 1)
    return words[start:end]


# =============================================================================
# CLI
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Error Context ‚Äî –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—à–∏–±–æ–∫'
    )

    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥—ã')

    # –ö–æ–º–∞–Ω–¥–∞ prepare
    prep_parser = subparsers.add_parser('prepare', help='–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ñ–∞–π–ª—ã')
    prep_parser.add_argument(
        'chapter',
        nargs='?',
        help='–ù–æ–º–µ—Ä –≥–ª–∞–≤—ã (1-5) –∏–ª–∏ --all –¥–ª—è –≤—Å–µ—Ö'
    )
    prep_parser.add_argument(
        '--all', '-a',
        action='store_true',
        help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –≥–ª–∞–≤—ã'
    )
    prep_parser.add_argument(
        '--force', '-f',
        action='store_true',
        help='–ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã'
    )

    # –ö–æ–º–∞–Ω–¥–∞ info
    info_parser = subparsers.add_parser('info', help='–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥–ª–∞–≤–µ')
    info_parser.add_argument('chapter', type=int, help='–ù–æ–º–µ—Ä –≥–ª–∞–≤—ã')

    args = parser.parse_args()

    print(f"Error Context v{VERSION}")
    print("=" * 60)

    if args.command == 'prepare':
        if args.all:
            chapters = [1, 2, 3, 4, 5]
        elif args.chapter:
            chapters = [int(args.chapter)]
        else:
            print("–£–∫–∞–∂–∏—Ç–µ –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã –∏–ª–∏ --all")
            return

        for chapter in chapters:
            print(f"\nüìñ –ì–ª–∞–≤–∞ {chapter}:")
            stats = prepare_chapter_files(chapter, force=args.force)

            if stats['files_created']:
                print(f"  ‚úÖ –°–æ–∑–¥–∞–Ω–æ: {len(stats['files_created'])} —Ñ–∞–π–ª–æ–≤")
                for f in stats['files_created']:
                    print(f"     - {Path(f).name}")

            if stats['files_skipped']:
                print(f"  ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ: {len(stats['files_skipped'])} —Ñ–∞–π–ª–æ–≤")

            if stats['errors']:
                print(f"  ‚ùå –û—à–∏–±–∫–∏:")
                for e in stats['errors']:
                    print(f"     - {e}")

    elif args.command == 'info':
        paths = get_chapter_paths(args.chapter)
        print(f"\nüìñ –ì–ª–∞–≤–∞ {args.chapter}:")

        for name, path in paths.items():
            if isinstance(path, Path):
                status = "‚úÖ" if path.exists() else "‚ùå"
                print(f"  {status} {name}: {path.name if path else 'N/A'}")

    else:
        parser.print_help()


if __name__ == '__main__':
    main()
