#!/usr/bin/env python3
"""
–§—Ä–µ–π–º–≤–æ—Ä–∫ –∞–Ω–∞–ª–∏–∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤ v1.0

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:
- –ü—Ä–æ–≥–æ–Ω—è–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —á–µ—Ä–µ–∑ –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
- –õ–æ–≥–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π —É—Ä–æ–≤–µ–Ω—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
- –°—á–∏—Ç–∞–µ—Ç: —Å–∫–æ–ª—å–∫–æ –æ—à–∏–±–æ–∫ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–ª –∫–∞–∂–¥—ã–π —É—Ä–æ–≤–µ–Ω—å
- –ü—Ä–æ–≤–µ—Ä—è–µ—Ç: –Ω–µ –∑–∞—Ü–µ–ø–∏–ª –ª–∏ —É—Ä–æ–≤–µ–Ω—å golden –æ—à–∏–±–∫–∏
- –°—Ç—Ä–æ–∏—Ç –º–∞—Ç—Ä–∏—Ü—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python –¢–µ—Å—Ç—ã/filter_analysis.py                    # –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π
    python –¢–µ—Å—Ç—ã/filter_analysis.py --chapter 1       # —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–∞ 1
    python –¢–µ—Å—Ç—ã/filter_analysis.py --transcript PATH  # –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
    python –¢–µ—Å—Ç—ã/filter_analysis.py --summary          # —Å–≤–æ–¥–∫–∞ –ø–æ –≤—Å–µ–º –ø—Ä–æ–≥–æ–Ω–∞–º
    python –¢–µ—Å—Ç—ã/filter_analysis.py --matrix           # –º–∞—Ç—Ä–∏—Ü–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

–í–µ—Ä—Å–∏—è: 1.0.0 (2026-01-30)
"""

VERSION = '1.0.0'
VERSION_DATE = '2026-01-30'

import argparse
import json
import os
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Set

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
PROJECT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_DIR / '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã'))

from config import (
    RESULTS_DIR, TESTS_DIR, TRANSCRIPTIONS_DIR,
    CHAPTERS_DIR, AUDIO_DIR, FileNaming
)


# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# =============================================================================

ANALYSIS_DIR = TESTS_DIR / '–ê–Ω–∞–ª–∏–∑_—Ñ–∏–ª—å—Ç—Ä–æ–≤'
GOLDEN_FILES = {
    '1': TESTS_DIR / '–∑–æ–ª–æ—Ç–æ–π_—Å—Ç–∞–Ω–¥–∞—Ä—Ç_–≥–ª–∞–≤–∞1.json',
    '2': TESTS_DIR / '–∑–æ–ª–æ—Ç–æ–π_—Å—Ç–∞–Ω–¥–∞—Ä—Ç_–≥–ª–∞–≤–∞2.json',
    '3': TESTS_DIR / '–∑–æ–ª–æ—Ç–æ–π_—Å—Ç–∞–Ω–¥–∞—Ä—Ç_–≥–ª–∞–≤–∞3.json',
    '4': TESTS_DIR / '–∑–æ–ª–æ—Ç–æ–π_—Å—Ç–∞–Ω–¥–∞—Ä—Ç_–≥–ª–∞–≤–∞4.json',
}

# –ü–æ—Ä—è–¥–æ–∫ —É—Ä–æ–≤–Ω–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–∏–∑ engine.py)
FILTER_LEVELS = [
    # –ó–∞—â–∏—Ç–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ (–≤–æ–∑–≤—Ä–∞—â–∞—é—Ç False = –ù–ï —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å)
    ('PROTECTED_hard_negative', -1, 'protection', '–ò–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–∞—Ä—ã –ø—É—Ç–∞–Ω–∏—Ü—ã'),
    ('PROTECTED_semantic_slip', -0.5, 'protection', '–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥–æ–≤–æ—Ä–∫–∏'),

    # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è
    ('morpho_same_form', 0, 'morpho', '–ò–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Ñ–æ—Ä–º—ã'),
    ('morpho_proper_name', 0, 'morpho', '–ò–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ'),

    # –†–∞–Ω–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞
    ('safe_ending_transition', 0.3, 'alignment', '–ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è'),
    ('yandex_phonetic_pair', 0.5, 'phonetic', '–§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä—ã –Ø–Ω–¥–µ–∫—Å–∞'),
    ('alignment_artifact', 0.6, 'alignment', '–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞)'),
    ('alignment_artifact_substring', 0.6, 'alignment', '–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ)'),

    # –≠—Ç–∞–ø 0: –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    ('alignment_start_artifact', 1, 'alignment', '–£–¥–∞–ª–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ (t=0)'),
    ('character_name_unrecognized', 1, 'names', '–ò–º–µ–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã'),
    ('split_name_insertion', 1, 'split', '–†–∞–∑–±–∏—Ç–æ–µ –∏–º—è'),
    ('split_name', 1, 'split', '–†–∞–∑–±–∏—Ç–æ–µ –∏–º—è (–¥–µ—Ç–µ–∫—Ç–æ—Ä)'),
    ('split_compound', 1, 'split', '–†–∞–∑–±–∏—Ç–æ–µ —Å–æ—Å—Ç–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ'),
    ('split_word_yandex', 1, 'split', '–Ø–Ω–¥–µ–∫—Å —Ä–∞–∑–±–∏–ª —Å–ª–æ–≤–æ'),
    ('split_suffix_insertion', 1, 'split', '–°—É—Ñ—Ñ–∏–∫—Å –∫–∞–∫ –≤—Å—Ç–∞–≤–∫–∞'),
    ('split_word_fragment', 1, 'split', '–§—Ä–∞–≥–º–µ–Ω—Ç —Ä–∞–∑–±–∏—Ç–æ–≥–æ —Å–ª–æ–≤–∞'),
    ('interrogative_split_to', 1, 'split', '–†–∞–∑–±–∏—Ç–æ–µ –¥–µ—Ñ–∏—Å–Ω–æ–µ (–∫—Ç–æ-—Ç–æ‚Üí–∫—Ç–æ —Ç–æ)'),
    ('compound_particle_to', 1, 'split', '–ß–∞—Å—Ç–∏—Ü–∞ "—Ç–æ"'),

    # –≠—Ç–∞–ø 3: –ú–µ–∂–¥–æ–º–µ—Ç–∏—è –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
    ('interjection', 3, 'weak', '–ú–µ–∂–¥–æ–º–µ—Ç–∏—è'),
    ('single_consonant_artifact', 3, 'alignment', '–û–¥–Ω–æ–±—É–∫–≤–µ–Ω–Ω—ã–µ —Å–æ–≥–ª–∞—Å–Ω—ã–µ'),
    ('misrecognition_artifact', 3, 'alignment', '–ü–æ—Ö–æ–∂–µ –Ω–∞ —Å–æ—Å–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ'),
    ('unknown_word_artifact', 3, 'alignment', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ (UNKN)'),
    ('rare_adverb', 3, 'weak', '–†–µ–¥–∫–∏–µ –Ω–∞—Ä–µ—á–∏—è'),
    ('sentence_start_weak', 3, 'weak', '–°–ª–∞–±–æ–µ —Å–ª–æ–≤–æ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è'),
    ('hyphenated_part', 3, 'split', '–ß–∞—Å—Ç—å –¥–µ—Ñ–∏—Å–Ω–æ–≥–æ —Å–ª–æ–≤–∞'),
    ('compound_word_part', 3, 'split', '–ß–∞—Å—Ç—å —Å–æ—Å—Ç–∞–≤–Ω–æ–≥–æ —Å–ª–æ–≤–∞'),

    # –≠—Ç–∞–ø 4: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ
    ('context_artifact', 4, 'context', '–ê—Ä—Ç–µ—Ñ–∞–∫—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞'),

    # –£—Ä–æ–≤–µ–Ω—å 1: –ó–∞—â–∏—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
    ('yandex_typical', 5, 'yandex', '–¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ –Ø–Ω–¥–µ–∫—Å–∞'),
    ('same_lemma', 5, 'morpho', '–û–¥–∏–Ω–∞–∫–æ–≤–∞—è –ª–µ–º–º–∞'),
    ('yandex_name_error', 5, 'names', '–û—à–∏–±–∫–∞ –≤ –∏–º–µ–Ω–∏'),
    ('levenshtein_protected', 5, 'phonetic', '–õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω ‚â§1 (–∑–∞—â–∏—â—ë–Ω–Ω—ã–µ)'),

    # –£—Ä–æ–≤–µ–Ω—å 2: –°–ª–∞–±—ã–µ —Å–ª–æ–≤–∞
    ('alignment_artifact', 6, 'weak', '–°–ª–∞–±—ã–µ DEL'),
    ('sentence_start_conjunction', 6, 'weak', '–°–æ—é–∑ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è'),
    ('split_word_insertion', 6, 'split', '–í—Å—Ç–∞–≤–∫–∞ —á–∞—Å—Ç–∏ —Å–ª–æ–≤–∞'),
    ('yandex_merge_artifact', 6, 'yandex', '–Ø–Ω–¥–µ–∫—Å —Å–ª–∏–ª —Å–ª–æ–≤–∞'),
    ('yandex_truncate_artifact', 6, 'yandex', '–Ø–Ω–¥–µ–∫—Å –æ–±—Ä–µ–∑–∞–ª'),
    ('yandex_expand_artifact', 6, 'yandex', '–Ø–Ω–¥–µ–∫—Å —Ä–∞—Å—à–∏—Ä–∏–ª'),
    ('yandex_i_ya_confusion', 6, 'phonetic', '–ü—É—Ç–∞–Ω–∏—Ü–∞ –∏‚Üî—è (–∫–æ–Ω—Ç–µ–∫—Å—Ç)'),
    ('yandex_i_ya_verb_context', 6, 'phonetic', '–ü—É—Ç–∞–Ω–∏—Ü–∞ –∏‚Üî—è (–≥–ª–∞–≥–æ–ª—ã)'),
    ('weak_words_identical', 6, 'weak', '–°–ª–∞–±—ã–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ'),
    ('weak_words_same_lemma', 6, 'weak', '–°–ª–∞–±—ã–µ –æ–¥–Ω–æ–π –ª–µ–º–º—ã'),

    # –£—Ä–æ–≤–µ–Ω—å 3: Substitution
    ('identical_normalized', 7, 'normalization', '–ò–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏'),
    ('homophone', 7, 'phonetic', '–û–º–æ—Ñ–æ–Ω—ã'),
    ('compound_word', 7, 'split', '–°–æ—Å—Ç–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ'),
    ('merged_word', 7, 'split', '–°–ª–∏—è–Ω–∏–µ —Å–ª–æ–≤'),
    ('case_form', 7, 'morpho', '–ü–∞–¥–µ–∂–Ω–∞—è —Ñ–æ—Ä–º–∞'),
    ('adverb_adjective', 7, 'morpho', '–ù–∞—Ä–µ—á–∏–µ‚Üî–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ'),
    ('short_full_adjective', 7, 'morpho', '–ö—Ä–∞—Ç–∫–æ–µ‚Üî–ø–æ–ª–Ω–æ–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ'),
    ('verb_gerund_safe', 7, 'morpho', '–ì–ª–∞–≥–æ–ª‚Üî–¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏–µ'),

    # –¶–µ–ø–æ—á–∫–∏
    ('alignment_chain', 8, 'chain', '–¶–µ–ø–æ—á–∫–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è'),
    ('linked_prefix_error', 8, 'chain', '–°–≤—è–∑–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏'),

    # ML
    ('ml_classifier', 10, 'ml', 'ML-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä'),

    # SmartFilter (–æ—Ç–∫–ª—é—á—ë–Ω)
    ('smart_filter', 11, 'smart', 'SmartFilter (—Å–∫–æ—Ä–∏–Ω–≥)'),
]

# –ì—Ä—É–ø–ø—ã —Ñ–∏–ª—å—Ç—Ä–æ–≤ –¥–ª—è —Å–≤–æ–¥–∫–∏
FILTER_GROUPS = {
    'protection': '–ó–∞—â–∏—Ç–Ω—ã–µ —Å–ª–æ–∏',
    'morpho': '–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è',
    'alignment': '–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è',
    'phonetic': '–§–æ–Ω–µ—Ç–∏–∫–∞',
    'split': '–†–∞–∑–±–∏—Ç—ã–µ —Å–ª–æ–≤–∞',
    'weak': '–°–ª–∞–±—ã–µ —Å–ª–æ–≤–∞',
    'names': '–ò–º–µ–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π',
    'yandex': '–¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ –Ø–Ω–¥–µ–∫—Å–∞',
    'context': '–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã',
    'chain': '–¶–µ–ø–æ—á–∫–∏ –æ—à–∏–±–æ–∫',
    'normalization': '–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è',
    'ml': 'ML-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä',
    'smart': 'SmartFilter',
}


# =============================================================================
# –ó–ê–ì–†–£–ó–ö–ê GOLDEN –°–¢–ê–ù–î–ê–†–¢–ê
# =============================================================================

def load_golden_errors(chapter_num: str) -> List[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç golden –æ—à–∏–±–∫–∏ –¥–ª—è –≥–ª–∞–≤—ã."""
    golden_file = GOLDEN_FILES.get(chapter_num)
    if not golden_file or not golden_file.exists():
        return []

    with open(golden_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    return data.get('errors', data) if isinstance(data, dict) else data


def normalize_for_comparison(word: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–≤–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    if not word:
        return ''
    return word.lower().strip().replace('—ë', '–µ')


def is_golden_error(error: Dict, golden_errors: List[Dict]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—à–∏–±–∫–∞ golden."""
    error_type = error.get('type', '')

    if error_type == 'substitution':
        orig = normalize_for_comparison(error.get('original', '') or error.get('correct', ''))
        trans = normalize_for_comparison(error.get('transcript', '') or error.get('wrong', ''))
    elif error_type == 'insertion':
        orig = ''
        trans = normalize_for_comparison(error.get('transcript', '') or error.get('wrong', '') or error.get('word', ''))
    elif error_type == 'deletion':
        orig = normalize_for_comparison(error.get('original', '') or error.get('correct', '') or error.get('word', ''))
        trans = ''
    else:
        return False

    error_time = error.get('time', 0)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Ä–µ–º—è –≤ float
    try:
        error_time = float(error_time) if error_time else 0.0
    except (ValueError, TypeError):
        error_time = 0.0

    for golden in golden_errors:
        g_type = golden.get('type', '')
        if g_type != error_type:
            continue

        if g_type == 'substitution':
            g_orig = normalize_for_comparison(golden.get('original', '') or golden.get('correct', ''))
            g_trans = normalize_for_comparison(golden.get('transcript', '') or golden.get('wrong', ''))
        elif g_type == 'insertion':
            g_orig = ''
            g_trans = normalize_for_comparison(golden.get('transcript', '') or golden.get('wrong', '') or golden.get('word', ''))
        elif g_type == 'deletion':
            g_orig = normalize_for_comparison(golden.get('original', '') or golden.get('correct', '') or golden.get('word', ''))
            g_trans = ''
        else:
            continue

        if orig == g_orig and trans == g_trans:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è (¬±5 —Å–µ–∫—É–Ω–¥)
            g_time = golden.get('time', 0)
            try:
                g_time = float(g_time) if g_time else 0.0
            except (ValueError, TypeError):
                g_time = 0.0
            if abs(error_time - g_time) <= 5:
                return True

    return False


# =============================================================================
# –ê–ù–ê–õ–ò–ó –§–ò–õ–¨–¢–†–ê–¶–ò–ò
# =============================================================================

def analyze_single_filter_pass(
    errors: List[Dict],
    golden_errors: List[Dict],
) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–∂–¥–æ–º—É —É—Ä–æ–≤–Ω—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
    """
    from filters import should_filter_error, filter_errors
    from filters.detectors import detect_alignment_chains, detect_linked_prefix_errors

    stats = defaultdict(lambda: {
        'filtered_count': 0,
        'golden_hit': 0,
        'golden_protected': 0,
        'examples': [],
    })

    # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º —Ü–µ–ø–æ—á–∫–∏
    chain_indices = detect_alignment_chains(errors)
    linked_prefix_indices = detect_linked_prefix_errors(errors)

    for idx, error in enumerate(errors):
        is_golden = is_golden_error(error, golden_errors)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ø–æ—á–∫–∏
        if idx in chain_indices:
            stats['alignment_chain']['filtered_count'] += 1
            if is_golden:
                stats['alignment_chain']['golden_hit'] += 1
            continue

        if idx in linked_prefix_indices:
            stats['linked_prefix_error']['filtered_count'] += 1
            if is_golden:
                stats['linked_prefix_error']['golden_hit'] += 1
            continue

        # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        should_filter, reason = should_filter_error(error, all_errors=errors)

        if reason.startswith('PROTECTED_'):
            # –ó–∞—â–∏—Ç–Ω—ã–π —Å–ª–æ–π ‚Äî –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç, –∞ –∑–∞—â–∏—â–∞–µ—Ç
            stats[reason]['golden_protected'] += 1 if is_golden else 0
            stats[reason]['filtered_count'] += 1  # —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ "–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ"
        elif should_filter:
            stats[reason]['filtered_count'] += 1
            if is_golden:
                stats[reason]['golden_hit'] += 1
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã golden, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã (–ö–†–ò–¢–ò–ß–ù–û!)
                stats[reason]['examples'].append({
                    'type': error.get('type'),
                    'original': error.get('original', error.get('correct', '')),
                    'transcript': error.get('transcript', error.get('wrong', '')),
                    'time': error.get('time'),
                })

    return dict(stats)


def run_full_analysis(
    transcript_path: str,
    original_path: str,
    chapter_num: str,
    output_dir: Path,
) -> Dict[str, Any]:
    """
    –ü—Ä–æ–≥–æ–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º.

    1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    2. –£–º–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    3. –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    """
    from smart_compare import smart_compare

    # –ó–∞–≥—Ä—É–∂–∞–µ–º golden
    golden_errors = load_golden_errors(chapter_num)

    # –®–∞–≥ 1: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    print(f"    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è...")
    # (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω –∏–ª–∏ –¥–µ–ª–∞–µ–º —ç—Ç–æ)

    # –®–∞–≥ 2: –£–º–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print(f"    –£–º–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ...")
    compared_path = output_dir / f'{chapter_num.zfill(2)}_analysis_compared.json'

    result = smart_compare(
        transcript_path=transcript_path,
        original_path=original_path,
        output_path=str(compared_path),
        force=True,
    )

    errors = result.get('errors', [])
    total_errors = len(errors)

    print(f"    –ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–ª–∏—á–∏–π: {total_errors}")

    # –®–∞–≥ 3: –ê–Ω–∞–ª–∏–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    print(f"    –ê–Ω–∞–ª–∏–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏...")
    filter_stats = analyze_single_filter_pass(errors, golden_errors)

    # –ü–æ–¥—Å—á—ë—Ç –∏—Ç–æ–≥–æ–≤
    total_filtered = sum(
        s['filtered_count'] for reason, s in filter_stats.items()
        if not reason.startswith('PROTECTED_')
    )
    total_golden_hit = sum(
        s['golden_hit'] for s in filter_stats.values()
    )
    total_protected = sum(
        s['golden_protected'] for s in filter_stats.values()
    )

    analysis = {
        'transcript': str(transcript_path),
        'original': str(original_path),
        'chapter': chapter_num,
        'timestamp': datetime.now().isoformat(),
        'golden_count': len(golden_errors),
        'total_differences': total_errors,
        'total_filtered': total_filtered,
        'remaining_errors': total_errors - total_filtered,
        'golden_hit': total_golden_hit,
        'golden_protected': total_protected,
        'filter_stats': filter_stats,
    }

    return analysis


def find_all_transcripts() -> List[Dict]:
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ –ø—Ä–æ–µ–∫—Ç–µ."""
    transcripts = []

    for chapter_dir in TRANSCRIPTIONS_DIR.iterdir():
        if not chapter_dir.is_dir() or not chapter_dir.name.startswith('–ì–ª–∞–≤–∞'):
            continue

        chapter_num = chapter_dir.name.replace('–ì–ª–∞–≤–∞', '').strip()

        for json_file in chapter_dir.glob('*.json'):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã
            if any(x in json_file.name for x in ['_compared', '_filtered', '_analysis']):
                continue

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            name = json_file.name
            if 'kbps' in name.lower():
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∏—Ç—Ä–µ–π—Ç
                import re
                match = re.search(r'(\d+)kbps', name, re.IGNORECASE)
                bitrate = match.group(1) if match else 'unknown'
                trans_type = f'{bitrate}kbps'
            elif 'yandex' in name.lower():
                trans_type = 'yandex'
            else:
                trans_type = 'standard'

            transcripts.append({
                'path': json_file,
                'chapter': chapter_num,
                'type': trans_type,
                'name': json_file.stem,
            })

    return sorted(transcripts, key=lambda x: (x['chapter'], x['type']))


def find_original_for_chapter(chapter_num: str) -> Optional[Path]:
    """–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ –¥–ª—è –≥–ª–∞–≤—ã."""
    variants = [
        f'–ì–ª–∞–≤–∞{chapter_num}.docx',
        f'–ì–ª–∞–≤–∞ {chapter_num}.docx',
        f'–ì–ª–∞–≤–∞_{chapter_num}.docx',
    ]

    for variant in variants:
        path = CHAPTERS_DIR / variant
        if path.exists():
            return path

    return None


# =============================================================================
# –û–¢–ß–Å–¢–´ –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
# =============================================================================

def print_analysis_report(analysis: Dict[str, Any]) -> None:
    """–í—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É."""
    print(f"\n{'='*70}")
    print(f"  –ê–ù–ê–õ–ò–ó: {Path(analysis['transcript']).name}")
    print(f"{'='*70}")
    print(f"  –ì–ª–∞–≤–∞: {analysis['chapter']}")
    print(f"  Golden –æ—à–∏–±–æ–∫: {analysis['golden_count']}")
    print(f"  –í—Å–µ–≥–æ —Ä–∞–∑–ª–∏—á–∏–π: {analysis['total_differences']}")
    print(f"  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {analysis['total_filtered']}")
    print(f"  –û—Å—Ç–∞–ª–æ—Å—å: {analysis['remaining_errors']}")
    print(f"  Golden –∑–∞—Ç—Ä–æ–Ω—É—Ç–æ: {analysis['golden_hit']} {'‚ö† –ö–†–ò–¢–ò–ß–ù–û!' if analysis['golden_hit'] > 0 else '‚úì'}")
    print(f"  Golden –∑–∞—â–∏—â–µ–Ω–æ: {analysis['golden_protected']}")
    print(f"\n  {'–§–∏–ª—å—Ç—Ä':<35} {'–û—Ç—Ñ–∏–ª—å—Ç—Ä.':<12} {'Golden':<8} {'–°—Ç–∞—Ç—É—Å'}")
    print(f"  {'-'*65}")

    stats = analysis['filter_stats']

    for reason, data in sorted(stats.items(), key=lambda x: -x[1]['filtered_count']):
        if reason.startswith('PROTECTED_'):
            status = f"–∑–∞—â–∏—Ç–∏–ª {data['golden_protected']}"
            count_str = f"({data['filtered_count']})"
        else:
            status = '‚úì' if data['golden_hit'] == 0 else f"‚ö† {data['golden_hit']}"
            count_str = str(data['filtered_count'])

        print(f"  {reason:<35} {count_str:<12} {data['golden_hit']:<8} {status}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö golden
        if data['golden_hit'] > 0 and data['examples']:
            for ex in data['examples'][:3]:
                print(f"      ‚Üí {ex['original']} ‚Üí {ex['transcript']} (t={ex['time']})")

    print(f"{'='*70}\n")


def build_effectiveness_matrix(analyses: List[Dict]) -> Dict[str, Any]:
    """
    –°—Ç—Ä–æ–∏—Ç –º–∞—Ç—Ä–∏—Ü—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤.

    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –ø—Ä–æ–≥–æ–Ω—ã –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç:
    - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã (–º–Ω–æ–≥–æ FP, 0 golden)
    - –ë–µ—Å–ø–æ–ª–µ–∑–Ω—ã–µ (0 –∏–ª–∏ –º–∞–ª–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–π)
    - –í—Ä–µ–¥–Ω—ã–µ (–∑–∞—Ç—Ä–∞–≥–∏–≤–∞—é—Ç golden)
    - –¢—Ä–µ–±—É—é—â–∏–µ –¥–æ—Ä–∞–±–æ—Ç–∫–∏
    """
    matrix = defaultdict(lambda: {
        'total_filtered': 0,
        'total_golden_hit': 0,
        'total_golden_protected': 0,
        'runs': 0,
        'effectiveness': 0.0,
        'status': 'unknown',
        'examples': [],
    })

    for analysis in analyses:
        for reason, data in analysis['filter_stats'].items():
            matrix[reason]['total_filtered'] += data['filtered_count']
            matrix[reason]['total_golden_hit'] += data['golden_hit']
            matrix[reason]['total_golden_protected'] += data.get('golden_protected', 0)
            matrix[reason]['runs'] += 1
            matrix[reason]['examples'].extend(data.get('examples', []))

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–∞–∂–¥–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞
    for reason, data in matrix.items():
        if reason.startswith('PROTECTED_'):
            data['status'] = 'protection'
            data['effectiveness'] = data['total_golden_protected']
        elif data['total_golden_hit'] > 0:
            data['status'] = 'harmful'
            data['effectiveness'] = -data['total_golden_hit']
        elif data['total_filtered'] == 0:
            data['status'] = 'useless'
            data['effectiveness'] = 0
        elif data['total_filtered'] < 5:
            data['status'] = 'low_impact'
            data['effectiveness'] = data['total_filtered']
        else:
            data['status'] = 'effective'
            data['effectiveness'] = data['total_filtered']

    return dict(matrix)


def print_effectiveness_matrix(matrix: Dict[str, Any]) -> None:
    """–í—ã–≤–æ–¥–∏—Ç –º–∞—Ç—Ä–∏—Ü—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."""
    print(f"\n{'#'*70}")
    print(f"  –ú–ê–¢–†–ò–¶–ê –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò –§–ò–õ–¨–¢–†–û–í")
    print(f"{'#'*70}")

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å—Ç–∞—Ç—É—Å—É
    by_status = defaultdict(list)
    for reason, data in matrix.items():
        by_status[data['status']].append((reason, data))

    status_order = ['harmful', 'protection', 'effective', 'low_impact', 'useless']
    status_labels = {
        'harmful': '‚ö† –í–†–ï–î–ù–´–ï (–∑–∞—Ç—Ä–∞–≥–∏–≤–∞—é—Ç Golden)',
        'protection': 'üõ° –ó–ê–©–ò–¢–ù–´–ï (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é Golden)',
        'effective': '‚úì –≠–§–§–ï–ö–¢–ò–í–ù–´–ï (—Ñ–∏–ª—å—Ç—Ä—É—é—Ç FP –±–µ–∑ –ø–æ—Ç–µ—Ä—å)',
        'low_impact': '‚óã –ù–ò–ó–ö–ò–ô –≠–§–§–ï–ö–¢ (< 5 —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–π)',
        'useless': '‚úó –ë–ï–°–ü–û–õ–ï–ó–ù–´–ï (0 —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–π)',
    }

    for status in status_order:
        items = by_status.get(status, [])
        if not items:
            continue

        print(f"\n  {status_labels[status]}")
        print(f"  {'-'*60}")

        for reason, data in sorted(items, key=lambda x: -abs(x[1]['effectiveness'])):
            if status == 'protection':
                print(f"    {reason:<35} –∑–∞—â–∏—Ç–∏–ª {data['total_golden_protected']} golden")
            else:
                print(f"    {reason:<35} {data['total_filtered']:>4} FP, {data['total_golden_hit']:>2} golden")
                if data['total_golden_hit'] > 0:
                    for ex in data['examples'][:2]:
                        print(f"        ‚Üí {ex['original']} ‚Üí {ex['transcript']}")

    print(f"\n{'#'*70}\n")


def save_analysis(analysis: Dict[str, Any], output_dir: Path) -> Path:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–∞–π–ª."""
    output_dir.mkdir(parents=True, exist_ok=True)

    transcript_name = Path(analysis['transcript']).stem
    filename = f"{transcript_name}_analysis.json"
    filepath = output_dir / filename

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)

    return filepath


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description=f'–ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤ v{VERSION}',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument('--chapter', '-c', choices=['1', '2', '3', '4'],
                        help='–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—É—é –≥–ª–∞–≤—É')
    parser.add_argument('--transcript', '-t', type=str,
                        help='–ü—É—Ç—å –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏')
    parser.add_argument('--summary', '-s', action='store_true',
                        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å–≤–æ–¥–∫—É –ø–æ –≤—Å–µ–º –ø—Ä–æ–≥–æ–Ω–∞–º')
    parser.add_argument('--matrix', '-m', action='store_true',
                        help='–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏')
    parser.add_argument('--skip-bitrate', action='store_true',
                        help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –±–∏—Ç—Ä–µ–π—Ç–æ–º (—Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ)')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
    parser.add_argument('--version', '-V', action='store_true',
                        help='–ü–æ–∫–∞–∑–∞—Ç—å –≤–µ—Ä—Å–∏—é')

    args = parser.parse_args()

    if args.version:
        print(f"filter_analysis v{VERSION} ({VERSION_DATE})")
        return 0

    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–∞–ª–∏–∑—ã –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã
    if args.summary or args.matrix:
        analyses = []
        for json_file in ANALYSIS_DIR.glob('*_analysis.json'):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    analyses.append(json.load(f))
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {json_file.name}: {e}")

        if not analyses:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ–≥–æ–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π.")
            return 1

        if args.matrix:
            matrix = build_effectiveness_matrix(analyses)
            print_effectiveness_matrix(matrix)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—É
            matrix_path = ANALYSIS_DIR / 'effectiveness_matrix.json'
            with open(matrix_path, 'w', encoding='utf-8') as f:
                json.dump(matrix, f, ensure_ascii=False, indent=2, default=str)
            print(f"–ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {matrix_path}")

        if args.summary:
            print(f"\n{'='*70}")
            print(f"  –°–í–û–î–ö–ê –ü–û {len(analyses)} –ü–†–û–ì–û–ù–ê–ú")
            print(f"{'='*70}")

            for analysis in sorted(analyses, key=lambda x: (x['chapter'], x['transcript'])):
                trans_name = Path(analysis['transcript']).name
                status = '‚úì' if analysis['golden_hit'] == 0 else '‚ö†'
                print(f"  {status} –ì–ª.{analysis['chapter']} {trans_name:<40} "
                      f"FP: {analysis['remaining_errors']:>3} / {analysis['total_differences']:>3}")

            print(f"{'='*70}\n")

        return 0

    # –ù–∞—Ö–æ–¥–∏–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if args.transcript:
        transcripts = [{'path': Path(args.transcript), 'chapter': '1', 'type': 'manual'}]
    else:
        transcripts = find_all_transcripts()

        if args.chapter:
            transcripts = [t for t in transcripts if t['chapter'] == args.chapter]

        if args.skip_bitrate:
            transcripts = [t for t in transcripts if 'kbps' not in t['type']]

    if not transcripts:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
        return 1

    print(f"\n{'#'*70}")
    print(f"  –ê–ù–ê–õ–ò–ó –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò –§–ò–õ–¨–¢–†–û–í v{VERSION}")
    print(f"{'#'*70}")
    print(f"  –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(transcripts)}")
    print(f"  –ü–∞–ø–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {ANALYSIS_DIR}")
    print(f"{'#'*70}\n")

    analyses = []

    for trans in transcripts:
        print(f"\n  –ê–Ω–∞–ª–∏–∑: {trans['path'].name}")
        print(f"  –ì–ª–∞–≤–∞: {trans['chapter']}, –¢–∏–ø: {trans['type']}")

        # –ù–∞—Ö–æ–¥–∏–º –æ—Ä–∏–≥–∏–Ω–∞–ª
        original = find_original_for_chapter(trans['chapter'])
        if not original:
            print(f"    ‚ö† –ù–µ –Ω–∞–π–¥–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª –¥–ª—è –≥–ª–∞–≤—ã {trans['chapter']}")
            continue

        try:
            analysis = run_full_analysis(
                transcript_path=str(trans['path']),
                original_path=str(original),
                chapter_num=trans['chapter'],
                output_dir=ANALYSIS_DIR,
            )

            print_analysis_report(analysis)

            save_path = save_analysis(analysis, ANALYSIS_DIR)
            print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {save_path.name}")

            analyses.append(analysis)

        except Exception as e:
            print(f"    ‚úó –û—à–∏–±–∫–∞: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()

    # –ò—Ç–æ–≥–æ–≤–∞—è –º–∞—Ç—Ä–∏—Ü–∞
    if len(analyses) > 0:
        matrix = build_effectiveness_matrix(analyses)
        print_effectiveness_matrix(matrix)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—É
        matrix_path = ANALYSIS_DIR / 'effectiveness_matrix.json'
        with open(matrix_path, 'w', encoding='utf-8') as f:
            json.dump(matrix, f, ensure_ascii=False, indent=2, default=str)
        print(f"–ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {matrix_path}")

    return 0


if __name__ == '__main__':
    sys.exit(main())
